{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14304e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] =  os.environ.get('LANGCHAIN_API_KEY')\n",
    "os.environ['GEMINI_API_KEY'] = os.environ.get('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4bfa064",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0807288c",
   "metadata": {},
   "source": [
    "## 4. Indexing\n",
    "### 4.1 Multi Representation Indexing works by : \n",
    "- generating propositions(summaries,big,main ideas) from documents\n",
    "- storing these propositions as embeddings (in vectorstore) indexed to full documenets(in docstore)\n",
    "- and on a user question it locates the relevant summary and retrieves the document for providing context to the llm\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e30486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff798cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "chain = (\n",
    "    {\"doc\":lambda x:x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document: \\n\\n{doc}\")\n",
    "    | ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",api_key=api_key,temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries = chain.batch(docs,{\"max_concurrency\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13bd3e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\",\n",
    "                     embedding_function=GoogleGenerativeAIEmbeddings(model=\"text-embedding-004\", google_api_key=api_key))\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# Docs linked to summaries\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# Add\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91fa16c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': '44c0cde1-30cd-46d9-8963-95db51b7917d'}, page_content=\"This document provides a comprehensive overview of LLM-powered autonomous agents, which leverage large language models as their core controllers for general problem-solving.\\n\\nThe architecture of these agents is built around three key components:\\n1.  **Planning:** Agents break down complex tasks into manageable subgoals (e.g., Chain of Thought, Tree of Thoughts) and employ self-reflection mechanisms (e.g., ReAct, Reflexion, Chain of Hindsight, Algorithm Distillation) to learn from past actions, correct mistakes, and refine future plans.\\n2.  **Memory:** This includes short-term memory (in-context learning within the LLM's finite context window) and long-term memory (external vector stores for infinite information retention and fast retrieval, often using Maximum Inner Product Search (MIPS) algorithms like HNSW or FAISS).\\n3.  **Tool Use:** Agents are equipped to call external APIs and modules (e.g., MRKL, TALM, Toolformer, ChatGPT Plugins, HuggingGPT, API-Bank) to access real-time information, execute code, or interact with specialized systems, significantly extending their capabilities beyond their training data.\\n\\nCase studies highlight applications such as scientific discovery (ChemCrow for chemistry tasks), generative agents simulating human behavior in sandbox environments, and proof-of-concept demos like AutoGPT and GPT-Engineer for autonomous task execution and code generation.\\n\\nDespite their potential, several challenges remain: finite context length limits the amount of historical information and instructions; long-term planning and robust error recovery are still difficult; and the reliability of natural language interfaces can lead to formatting errors or unexpected behavior.\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Memory in agents\"\n",
    "sub_docs = vectorstore.similarity_search(query,k=1)\n",
    "sub_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c88084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(query,n_results=1)\n",
    "retrieved_docs[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ce2cc9",
   "metadata": {},
   "source": [
    "## 4.2 RAPTOR\n",
    "\n",
    "- Full form → Recursive Abstractive Processing for Tree-Organized Retrieval.\n",
    "\n",
    "- Purpose → Makes retrieval more effective on long or complex documents by creating a hierarchical index.\n",
    "\n",
    "- How it works:\n",
    "\n",
    "    - Chunking — break large documents into small passages.\n",
    "    - Recursive summarization — group passages into clusters and generate summaries at multiple levels (passage → section → document).\n",
    "    - Tree structure — these summaries form a hierarchical tree, where higher nodes represent broader concepts.\n",
    "    - Retrieval — when a query comes in, search can happen at different levels of the tree:\n",
    "        - Directly retrieve relevant chunks\n",
    "        - Or retrieve summaries that lead to deeper chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3711e",
   "metadata": {},
   "source": [
    "## 4.3 ColBERT\n",
    "- How it works:\n",
    "\n",
    "    - Token-level embeddings — instead of compressing a whole document into a single vector, ColBERT keeps embeddings for each token (or subword).\n",
    "\n",
    "    - Late interaction — at query time, it computes similarity between query tokens and document tokens using MaxSim (max similarity across token pairs).\n",
    "\n",
    "    - Efficiency trick — stores all token embeddings compressed, so retrieval is still fast and scalable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
