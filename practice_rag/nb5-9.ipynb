{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daee90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_community tiktoken langchain-google-genai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02b42eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] =  os.environ.get('LANGCHAIN_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c8067eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GEMINI_API_KEY'] = os.environ.get('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe6d06",
   "metadata": {},
   "source": [
    "## 1.1 Multi Query\n",
    "### <i>a technique where an initial user query is transformed into multiple, distinct queries, typically to enhance the retrieval of relevant information. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019a785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b926597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings \n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "\n",
    "api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits, \n",
    "    embedding=GoogleGenerativeAIEmbeddings(model=\"text-embedding-004\", google_api_key=api_key)\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b764bc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",google_api_key=api_key, temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008fb7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bdde757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents is the process where the agent breaks down large, complicated tasks into smaller, more manageable subgoals or steps. This enables the efficient handling of complex tasks and allows the agent to plan ahead.\\n\\nCommon techniques for task decomposition include:\\n*   **Chain of Thought (CoT) prompting**: Instructing the LLM to \"think step by step\" to decompose hard tasks into simpler steps.\\n*   **Tree of Thoughts**: Extending CoT by exploring multiple reasoning possibilities at each step, generating multiple thoughts per step in a tree structure.\\n\\nTask decomposition can be achieved by:\\n*   LLM with simple prompting (e.g., \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\").\\n*   Using task-specific instructions (e.g., \"Write a story outline.\").\\n*   Human inputs.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "#RAG\n",
    "template = \"\"\"Answer the following quetsion based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question:{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",google_api_key=api_key,temperature=0)\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fef204",
   "metadata": {},
   "source": [
    "## 1.2 RAG FUSION\n",
    "### <i>RAG-Fusion is an advanced information retrieval technique that enhances Retrieval-Augmented Generation (RAG) by generating multiple queries from an initial user query, retrieving relevant documents for each, and then using Reciprocal Rank Fusion (RRF) to combine and re-rank the results, ultimately producing more accurate and comprehensive answers from a large language model (LLM).</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7770acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are a helpful assistant who generates multiple search queries based on a single input query.\\n \n",
    "Generate multiple search queries related to : {question} \\n\n",
    "Output (4 queries) :\"\"\"\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba9aaa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.environ.get(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9b38f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",google_api_key=api_key,temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x : x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "616eb91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps,loads\n",
    "\n",
    "def reciprocal_rank_fusion(results : list[list],k=60):\n",
    "    \"\"\"Reciprocal_rank_fusion that takes multiple list of ranked documents\n",
    "    and an optional paramter k used in RRF formula\"\"\"\n",
    "    \n",
    "    fused_scores = {}\n",
    "    \n",
    "    for docs in results:\n",
    "        for rank,doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1/(rank+k)\n",
    "    reranked_results = [\n",
    "        (loads(doc),score)\n",
    "        for doc,score in sorted(fused_scores.items(),key=lambda x : x[1],reverse=True)\n",
    "    ]\n",
    "    \n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36585a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents is the process of breaking down large, complex tasks into smaller, more manageable subgoals or steps. This is a crucial part of the agent\\'s planning component, enabling it to handle complex tasks efficiently and enhance its performance.\\n\\nKey aspects of task decomposition include:\\n*   **Purpose:** It transforms big tasks into multiple manageable tasks, allowing the model to utilize more computation and providing insight into its thinking process.\\n*   **Techniques:**\\n    *   **Chain of Thought (CoT):** A standard prompting technique where the model is instructed to \"think step by step\" to decompose hard tasks into simpler ones.\\n    *   **Tree of Thoughts (ToT):** An extension of CoT that explores multiple reasoning possibilities at each step, generating multiple thoughts per step and creating a tree structure.\\n*   **Methods of Implementation:**\\n    *   Using the LLM itself with simple prompts (e.g., \"Steps for XYZ,\" \"What are the subgoals for achieving XYZ?\").\\n    *   Employing task-specific instructions (e.g., \"Write a story outline\" for a novel-writing task).\\n    *   Incorporating human inputs.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519291b",
   "metadata": {},
   "source": [
    "## 1.3 DECOMPOSITION\n",
    "### <i>query decomposition is a query transformation technique that addresses complex user queries by breaking them down into simpler, more manageable sub-questions or steps<i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ceff5a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11db32fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 Internal error encountered..\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",google_api_key=api_key,temperature=0)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f191ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are 3 search queries related to the main components of an LLM-powered autonomous agent system:',\n",
       " '',\n",
       " '1.  \"Architecture of LLM-powered autonomous agents\"',\n",
       " '2.  \"Key components of LLM agent systems\"',\n",
       " '3.  \"LLM agent design patterns and modules\"']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# RAG prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \n",
    "                                                                \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "129738b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An LLM-powered autonomous agent system is primarily controlled by a Large Language Model (LLM) at its core. These systems are typically composed of several key components that enable them to perceive, plan, act, and learn.\\n\\nThe main components include:\\n\\n1.  **Planning:** This component allows the agent to break down complex tasks into manageable steps, strategize, and refine its actions. It often involves mechanisms for self-reflection, enabling the agent to learn from past experiences and adjust its future plans, as seen in approaches like ReAct (synergizing reasoning and acting) and Reflexion.\\n2.  **Memory:** Essential for retaining and recalling information over various durations.\\n    *   **Short-Term Memory:** For immediate context and ongoing tasks.\\n    *   **Long-Term Memory (LTM):** Stores information for extended periods (days to decades) with essentially unlimited capacity. For agents, this is often implemented as an external vector store or a \"memory stream\" that records experiences in natural language, allowing for fast retrieval and enabling agents to behave based on past experiences.\\n3.  **Tool Use:** This capability allows the agent to interact with the external world beyond its internal knowledge. It enables the agent to call external APIs, access external knowledge sources, or utilize specific functionalities to gather additional information or perform actions. MRKL Systems, for instance, combine LLMs with external knowledge sources and discrete reasoning.\\n4.  **Reflection:** Often integrated with planning, reflection mechanisms allow agents to critically evaluate their past actions and outcomes, learn from mistakes, and improve their future decision-making and planning processes.\\n\\nArchitectures like AutoGPT and \"The generative agent architecture\" (Park et al. 2023) demonstrate how these components—LLM, memory, planning, and reflection—are combined to create agents capable of emergent behaviors and complex interactions.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":context,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54d4e57",
   "metadata": {},
   "source": [
    "## 1.4 STEP BACK\n",
    "### <i> a technique where the AI generates a more general, higher-level version of the original user question.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e640a0ed",
   "metadata": {},
   "source": [
    "## 1.5 HYDE\n",
    "### <i>a query transformation technique that improves retrieval by generating a hypothetical, well-structured document in response to a user's query. This \"fake\" document, created by a large language model (LLM), is then embedded and used for similarity search against the knowledge base's document embeddings, bridging the gap between short, informal queries and the longer, formal format of actual documents. </i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
