{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60597413",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain youtube-transcript-api pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76cc950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] =  os.environ.get('LANGCHAIN_API_KEY')\n",
    "os.environ['GEMINI_API_KEY'] = os.environ.get('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52fcc84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63462b4",
   "metadata": {},
   "source": [
    "## 2. Routing\n",
    "#### After query gtransformation it is important to send that query to the mist relevant datasource/prompt. That is done through Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81e082",
   "metadata": {},
   "source": [
    "## 2.1 Logical routing\n",
    "### <i> Letting the LLM decide which datasource out of the given options to point to / load depending on the question</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0059b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel,Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "    \n",
    "    datasource : Literal[\"python_docs\",\"js_docs\",\"golang_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"\"\"Given a user question, choose which datasource would be most relevant for answering their question\"\"\"\n",
    "    )\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",api_key=api_key,temperature=0)\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "#Prompt\n",
    "system = \"\"\"You are an expert at routing a user question to the relevant datsrource.\n",
    "Based on the programming language the question os referring to, route it to the relevant data source.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system),\n",
    "        (\"human\",\"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e04ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1902f757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='python_docs'\n",
      "python_docs\n"
     ]
    }
   ],
   "source": [
    "print(result)\n",
    "print(result.datasource)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0dbef7",
   "metadata": {},
   "source": [
    "once that is developed it is importnat to complete the process by defining a branch that will use the `result.datasource`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4069558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(result):\n",
    "    if result.datasource == \"python_docs\":\n",
    "        return \"chain for python docs\"\n",
    "    elif result.datasource == \"js_docs\":\n",
    "        return \"chain for js docs\"\n",
    "    else :\n",
    "        return \"chain for golang docs\"\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "409aad90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chain for python docs'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae982e8",
   "metadata": {},
   "source": [
    "## 2.2 Semantic Routing\n",
    "### <i> Semantic Routing in LangChain refers to the process of intelligently directing user queries to the most appropriate chain, tool, or prompt based on the meaning or intent of the input, rather than relying on explicit keywords or rule-based logic.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4334d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI,GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Two prompts\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "#Embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"text-embedding-004\", google_api_key=api_key)\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "745e33d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS\n",
      "A black hole is a region of spacetime where gravity is so incredibly strong that nothing, not even light, can escape from it.\n",
      "\n",
      "This extreme gravity arises from a huge amount of matter being compressed into an extremely small space. Imagine taking something many times more massive than our Sun and squeezing it down to the size of a city – that's the kind of density we're talking about.\n",
      "\n",
      "The \"point of no return\" around a black hole is called the **event horizon**. Once anything crosses this boundary, it's trapped forever. At the very center, all that mass is thought to be concentrated into an infinitely dense point called a **singularity**.\n"
     ]
    }
   ],
   "source": [
    "#Route questioon to prompt\n",
    "def prompt_router(input):\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    similarity = cosine_similarity([query_embedding],prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    \n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "chain = (\n",
    "    {\"query\":RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"What is a black hole.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c78d9",
   "metadata": {},
   "source": [
    "## 3. Query Construction\n",
    "Query construction is done to convert the natural language into domain-specific language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285dbf08",
   "metadata": {},
   "source": [
    "# Query Construction in RAG\n",
    "\n",
    "1. **Collect docs & metadata** — load transcripts and metadata (title, views, dates, length).  \n",
    "2. **Define schema** — design a structured query model (text fields + filters).  \n",
    "3. **Prompt LLM** — instruct the LLM to convert user questions into schema fields.  \n",
    "4. **LLM → structured output** — parse queries into text search + metadata filters.  \n",
    "5. **Normalize values** — convert to standard formats (dates, counts, durations).  \n",
    "6. **Map fields → vectorstore** — match content queries to embeddings, filters to metadata.  \n",
    "7. **Run filtered search** — execute similarity + filter queries in the vectorstore.  \n",
    "8. **Aggregate & rank** — combine and re-rank retrieved chunks.  \n",
    "9. **Generate final answer** — feed results + question to the LLM for response.  \n",
    "10. **Log & iterate** — record queries and outputs for refinement.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
